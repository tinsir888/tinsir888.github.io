<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>计算机视觉 W47 Sequence models (natural language processing) | tinsir888s hjemmeside</title><meta name="author" content="tinsir888"><meta name="copyright" content="tinsir888"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Sequence Models  sequence models are the machine learning models that input or output sequences of data sequential data includes text streams, audio"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://tinsir888.github.io/posts/e7117333.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload='this.media="all"'><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"/search.xml",preload:!0,top_n_per_article:1,unescape:!1,languages:{hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found"}},translate:void 0,noticeOutdate:void 0,highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:!1},copy:{success:"Copy Successful",error:"Copy Error",noSupport:"Browser Not Supported"},relativeDate:{homepage:!1,post:!1},runtime:"",dateSuffix:{just:"Just now",min:"minutes ago",hour:"hours ago",day:"days ago",month:"months ago"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js",buttonText:"Load More"},isPhotoFigcaption:!1,islazyload:!1,isAnchor:!1,percent:{toc:!0,rightside:!1},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"计算机视觉 W47 Sequence models (natural language processing)",isPost:!0,isHome:!1,isHighlightShrink:!0,isToc:!0,postUpdate:"2024-01-26 11:51:45"}</script><script>(e=>{e.saveToLocal={set:(e,t,o)=>{if(0===o)return;const a={value:t,expiry:Date.now()+864e5*o};localStorage.setItem(e,JSON.stringify(a))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!(Date.now()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=(e,t={})=>new Promise((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},Object.keys(t).forEach(e=>{n.setAttribute(e,t[e])}),document.head.appendChild(n)}),e.getCSS=(e,t=!1)=>new Promise((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onerror=a,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,o())},document.head.appendChild(n)}),e.activateDarkMode=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme");"dark"===t?activateDarkMode():"light"===t&&activateLightMode();const o=saveToLocal.get("aside-status");void 0!==o&&("hide"===o?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="tinsir888s hjemmeside" type="application/atom+xml">
</head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/favicon.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">316</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">74</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">32</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fa-regular fa-handshake"></i><span> 接力</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://www.travellings.cn/go.html"><i class="fa-fw fas fa-subway"></i><span> 开往</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://travel.moe/go?travel=on"><i class="fa-fw fas fa-rocket"></i><span> 异次元</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url(/img/AUcv.jpg)"><nav id="nav"><span id="blog-info"><a href="/" title="tinsir888s hjemmeside"><span class="site-name">tinsir888s hjemmeside</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fa-regular fa-handshake"></i><span> 接力</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://www.travellings.cn/go.html"><i class="fa-fw fas fa-subway"></i><span> 开往</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://travel.moe/go?travel=on"><i class="fa-fw fas fa-rocket"></i><span> 异次元</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">计算机视觉 W47 Sequence models (natural language processing)</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2023-11-19T23:00:00.000Z" title="Created 2023-11-20 00:00:00">2023-11-20</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2024-01-26T10:51:45.742Z" title="Updated 2024-01-26 11:51:45">2024-01-26</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AUDatalogi/">AUDatalogi</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AUDatalogi/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/">计算机视觉</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word Count:</span><span class="word-count">1.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading Time:</span><span>10mins</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="计算机视觉 W47 Sequence models (natural language processing)"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="sequence-models"><a class="markdownIt-Anchor" href="#sequence-models"></a> Sequence Models</h1><ul><li>sequence models are the machine learning models that input or output sequences of data</li><li>sequential data includes text streams, audio clips, video clips, time-series data etc.</li></ul><h1 id="nlp"><a class="markdownIt-Anchor" href="#nlp"></a> NLP</h1><p>technology to handle human language using computers</p><p>aid human-human communication and/or human-machine communication</p><p>analyze/understand text (syntactic analysis, text classification, named entity recognition, building knowledge graphs).</p><h2 id="language-model-next-word-prediction"><a class="markdownIt-Anchor" href="#language-model-next-word-prediction"></a> Language model ≈ next word prediction</h2><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>X</mi><mo>=</mo><mi>s</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>n</mi><mi>c</mi><mi>e</mi><mo stretchy="false">)</mo><mo>=</mo><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>t</mi></munderover><mi>P</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>⋯</mo><mtext> </mtext><mo separator="true">,</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo><mspace linebreak="newline"></mspace><mi>P</mi><mo stretchy="false">(</mo><mi>Y</mi><mo>=</mo><mi>s</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>n</mi><mi>c</mi><mi>e</mi><mi mathvariant="normal">∣</mi><mi>X</mi><mo>=</mo><mi>a</mi><mi>d</mi><mi>d</mi><mi>e</mi><mi>d</mi><mtext> </mtext><mi>c</mi><mi>o</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi><mo stretchy="false">)</mo><mo>=</mo><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>t</mi></munderover><mi>P</mi><mo stretchy="false">(</mo><msub><mi>y</mi><mi>i</mi></msub><mi mathvariant="normal">∣</mi><mi>X</mi><mo separator="true">,</mo><msub><mi>y</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>⋯</mo><mtext> </mtext><mo separator="true">,</mo><msub><mi>y</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo><mspace linebreak="newline"></mspace></mrow><annotation encoding="application/x-tex">P(X=sentence)=\prod_{i=1}^tP(x_i|x_1,\cdots,x_{i-1})\\ P(Y=sentence|X=added\ context)=\prod_{i=1}^tP(y_i|X,y_1,\cdots,y_{i-1})\\</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.13889em">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:.07847em">X</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal">s</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord mathnormal">c</span><span class="mord mathnormal">e</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:3.05823em;vertical-align:-1.277669em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.7805610000000003em"><span style="top:-1.872331em;margin-left:0"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.050005em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∏</span></span></span><span style="top:-4.3000050000000005em;margin-left:0"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.277669em"><span></span></span></span></span></span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal" style="margin-right:.13889em">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31166399999999994em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="minner">⋯</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.311664em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.208331em"><span></span></span></span></span></span></span><span class="mclose">)</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.13889em">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:.22222em">Y</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal">s</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord mathnormal">c</span><span class="mord mathnormal">e</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:.07847em">X</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal">a</span><span class="mord mathnormal">d</span><span class="mord mathnormal">d</span><span class="mord mathnormal">e</span><span class="mord mathnormal">d</span><span class="mspace"> </span><span class="mord mathnormal">c</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mord mathnormal">x</span><span class="mord mathnormal">t</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:3.05823em;vertical-align:-1.277669em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.7805610000000003em"><span style="top:-1.872331em;margin-left:0"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.050005em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∏</span></span></span><span style="top:-4.3000050000000005em;margin-left:0"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.277669em"><span></span></span></span></span></span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal" style="margin-right:.13889em">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31166399999999994em"><span style="top:-2.5500000000000003em;margin-left:-.03588em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:.07847em">X</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:-.03588em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="minner">⋯</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.311664em"><span style="top:-2.5500000000000003em;margin-left:-.03588em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.208331em"><span></span></span></span></span></span></span><span class="mclose">)</span></span><span class="mspace newline"></span></span></span></span></p><h2 id="word-embeddings"><a class="markdownIt-Anchor" href="#word-embeddings"></a> Word embeddings</h2><p>words <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>y</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">x_i,y_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31166399999999994em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31166399999999994em"><span style="top:-2.5500000000000003em;margin-left:-.03588em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span> are represented as vectors, called word embeddings.</p><p>basic idea: words that appear in similar context should have similar embeddings</p><p>e.g. “good” and “great” should have similar embeddings</p><p>word embeddings are learned in many models, such as Word2Vec, GloVe, ELMo etc.</p><h2 id="sentence-embeddings"><a class="markdownIt-Anchor" href="#sentence-embeddings"></a> Sentence embeddings</h2><p>just like we can embed words, we can also embed complete sentences.</p><p>different approach because, unlike words, we can not define a finite set of sentence.</p><h2 id="basic-modeling-paradigm"><a class="markdownIt-Anchor" href="#basic-modeling-paradigm"></a> Basic modeling paradigm</h2><p>given an input text, extract features (embedding), and predict label.</p><p>text classification - one label for the whole sentence</p><p>sequence labeling - one label for every word</p><h1 id="some-sequence-model"><a class="markdownIt-Anchor" href="#some-sequence-model"></a> Some sequence model</h1><p>multi-layer preceptron will not work for sentences, because the labels depend on the context</p><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover accent="true"><mi>y</mi><mo>^</mo></mover><mi>t</mi></msub><mo>=</mo><msub><mi>f</mi><mrow><mi>M</mi><mi>L</mi><mi>P</mi></mrow></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\hat y_t=f_{MLP}(x_t)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8888799999999999em;vertical-align:-.19444em"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.69444em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal" style="margin-right:.03588em">y</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-.19444em"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.19444em"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.2805559999999999em"><span style="top:-2.5500000000000003em;margin-left:-.03588em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.10764em">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.32833099999999993em"><span style="top:-2.5500000000000003em;margin-left:-.10764em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:.10903em">M</span><span class="mord mathnormal mtight">L</span><span class="mord mathnormal mtight" style="margin-right:.13889em">P</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.2805559999999999em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> will not work.</p><h2 id="recurrent-neural-network"><a class="markdownIt-Anchor" href="#recurrent-neural-network"></a> Recurrent Neural Network</h2><p>abbr. RNN</p><p>key idea: introducing an internal/hidden state that is updated as the sequence is processed.</p><p>apply recurrence formula (same function and same set of parameters at every time step).</p><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mi>t</mi></msub><mo>=</mo><msub><mi>f</mi><mrow><mi>R</mi><mi>N</mi><mi>N</mi></mrow></msub><mo stretchy="false">(</mo><msub><mi>h</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo separator="true">,</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">h_t=f_{RNN}(h_{t-1},x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.84444em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.2805559999999999em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.10764em">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.32833099999999993em"><span style="top:-2.5500000000000003em;margin-left:-.10764em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:.00773em">R</span><span class="mord mathnormal mtight" style="margin-right:.10903em">N</span><span class="mord mathnormal mtight" style="margin-right:.10903em">N</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.301108em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.208331em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></p><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover accent="true"><mi>y</mi><mo>^</mo></mover><mi>t</mi></msub><mo>=</mo><msub><mi>f</mi><mrow><mi>M</mi><mi>L</mi><mi>P</mi></mrow></msub><mo stretchy="false">(</mo><msub><mi>h</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\hat y_t=f_{MLP}(h_t)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8888799999999999em;vertical-align:-.19444em"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.69444em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal" style="margin-right:.03588em">y</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-.19444em"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.19444em"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.2805559999999999em"><span style="top:-2.5500000000000003em;margin-left:-.03588em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.10764em">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.32833099999999993em"><span style="top:-2.5500000000000003em;margin-left:-.10764em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:.10903em">M</span><span class="mord mathnormal mtight">L</span><span class="mord mathnormal mtight" style="margin-right:.13889em">P</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.2805559999999999em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></p><h3 id="training-rnns"><a class="markdownIt-Anchor" href="#training-rnns"></a> Training RNNs</h3><p>words -&gt; word embedder -&gt; RNN -&gt; MLP -&gt; predicted labels</p><p>loss = predicted labels - true labels</p><ul><li>same RNN/MLP being applied at every timestep</li><li>training is hard/slow.</li></ul><p>Solution</p><ul><li>use an extension of RNNs called Long Short-Term Memory (LSTM) networks</li><li>LSTM have memory allowing them to preserve information over many timesteps</li><li>Similar to ResNet</li></ul><h3 id="sentence-embeddings-with-rnnlstm"><a class="markdownIt-Anchor" href="#sentence-embeddings-with-rnnlstm"></a> Sentence embeddings with RNN/LSTM</h3><p>use the last hidden state as encoding of whole sentence/sequence</p><p>applications include sentence classification</p><p>can also use sentence embedding for text retrieval, sentence comparison etc.</p><p><strong>input could also be image embeddings</strong></p><ul><li>action recognition in video</li><li>embed each frame using CNN, then use these embeddings instead of word embeddings.</li></ul><h2 id="language-modeling"><a class="markdownIt-Anchor" href="#language-modeling"></a> Language modeling</h2><p>language modeling is like a sequence labeling task, where each label is the next word!</p><p>this is an autoregressive model (output from previous timestep used as input in current timestep).</p><h2 id="sequence-to-sequence-models"><a class="markdownIt-Anchor" href="#sequence-to-sequence-models"></a> Sequence-to-sequence models</h2><p>translation is not sequence labeling because imput sentence and output sentence could have different lengths</p><p>while input sequence must be encoded into a single vector <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mrow><mi>e</mi><mi>n</mi><mi>c</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>r</mi></mrow></msub></mrow><annotation encoding="application/x-tex">h_{encoder}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.84444em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.33610799999999996em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right:.02778em">r</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>.</p><p>decoder = conditional LM:</p><ul><li>it is autoregressive and predicts the next token</li><li>note that it is conditioned on <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mrow><mi>e</mi><mi>n</mi><mi>c</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>r</mi></mrow></msub></mrow><annotation encoding="application/x-tex">h_{encoder}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.84444em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.33610799999999996em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right:.02778em">r</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>.</li></ul><p><strong>conditional LM can also be used in image captioning</strong></p><ul><li>we could train a LM to be conditioned on anything really</li></ul><h3 id="problem-with-seq2seq-models"><a class="markdownIt-Anchor" href="#problem-with-seq2seq-models"></a> Problem with Seq2seq models</h3><p>You can’t cram the meaning of a whole f**king sentence into a single f**king vector - Ray Monney.</p><h2 id="solution-attention"><a class="markdownIt-Anchor" href="#solution-attention"></a> Solution: Attention</h2><p>for each word in the output sentence is identify the important and relevant words from the input sentence and assign higher weights to these words, enhancing the accuracy of the output prediction.</p><p>识别重要的单词（集中注意力）</p><p>Basic idea</p><ul><li>encode each word in the input sentence into a vector</li><li>when decoding, perform a linear combination of these vectors, weighted by attention weights</li><li>use this combination in picking the next word</li></ul><p>Key/value/query concept:</p><p>analogous to retrieval system. when searching for videos on YouTube, the search engine will map query against a set of keys associated with candidate videos in their database, then present the best matched videos (values).</p><p>start by matching the query against all keys (using dot product). Makes sense that “ich” gets the highest score, because it’s German for “I”.</p><p>Contextualized word embedding = linear combination （这个线性组合的系数实际上应该是 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>Key vectors</mtext><mo>⋅</mo><mtext>Query vectors</mtext></mrow><annotation encoding="application/x-tex">\text{Key vectors}\cdot\text{Query vectors}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8777699999999999em;vertical-align:-.19444em"></span><span class="mord text"><span class="mord">Key vectors</span></span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.8777699999999999em;vertical-align:-.19444em"></span><span class="mord text"><span class="mord">Query vectors</span></span></span></span></span> 的结果）of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>Value vectors</mtext></mrow><annotation encoding="application/x-tex">\text{Value vectors}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord text"><span class="mord">Value vectors</span></span></span></span></span></p><p>With attention we use all words of the input to predict the next token, not just depending on <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mrow><mi>e</mi><mi>n</mi><mi>c</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>r</mi></mrow></msub></mrow><annotation encoding="application/x-tex">h_{encoder}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.84444em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.33610799999999996em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right:.02778em">r</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>.</p><h3 id="attention-in-image-captioning"><a class="markdownIt-Anchor" href="#attention-in-image-captioning"></a> Attention in image Captioning</h3><p>features 可以表示成 keys and values 矩阵。</p><p>keys 矩阵变换成 attention weights</p><p>values 变换成 contextual image embedding</p><p>注意力机制在图像处理上也能起到一些着重强调重要像素的作用。</p><h3 id="self-attention"><a class="markdownIt-Anchor" href="#self-attention"></a> Self-attention</h3><p>each element in the sentence attends to other elements - context sensitive encodings</p><p>RNN/LSTM have an important limitation: each sequence must be treated one element at a time. Both the encoder and the decoder have to wait till the completion of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">t-1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69841em;vertical-align:-.08333em"></span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">1</span></span></span></span> steps to process the t-th step. So, when dealing with huge corpus it is very time consuming and computationally inefficient.</p><h2 id="transformers"><a class="markdownIt-Anchor" href="#transformers"></a> Transformers</h2><p>seq2seq model based entirely on attention (fast!!!)</p><p>self-attention: each laayer combines words with others.</p><p>multi-headed attention: 8 attention heads learned independently.</p><p>scaled dot-product attention: remove bias in dot product when using large networks</p><p>positional encodings: make sure that even if we don’t have RNN, can still distinguish position</p><h3 id="encoder-block"><a class="markdownIt-Anchor" href="#encoder-block"></a> Encoder block</h3><p>first encoder block receives a seq of word embeddings as input</p><p>remaining encoder blocks receive output from previous encoder block as input.</p><h3 id="self-attention-block"><a class="markdownIt-Anchor" href="#self-attention-block"></a> Self-attention block</h3><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>A</mi><mi>t</mi><mi>t</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo stretchy="false">(</mo><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi><mo stretchy="false">)</mo><mo>=</mo><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo stretchy="false">)</mo><mi>V</mi><mspace linebreak="newline"></mspace><msub><mi>q</mi><mi>i</mi></msub><mo>=</mo><msub><mi>W</mi><mi>q</mi></msub><msub><mi>x</mi><mi>i</mi></msub><mspace linebreak="newline"></mspace><msub><mi>k</mi><mi>i</mi></msub><mo>=</mo><msub><mi>W</mi><mi>k</mi></msub><msub><mi>x</mi><mi>i</mi></msub><mspace linebreak="newline"></mspace><msub><mi>v</mi><mi>i</mi></msub><mo>=</mo><msub><mi>W</mi><mi>v</mi></msub><msub><mi>x</mi><mi>i</mi></msub><mspace linebreak="newline"></mspace></mrow><annotation encoding="application/x-tex">Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V\\ q_i=W_qx_i\\ k_i=W_kx_i\\ v_i=W_vx_i\\</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal">A</span><span class="mord mathnormal">t</span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal" style="margin-right:.07153em">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal" style="margin-right:.22222em">V</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:2.448331em;vertical-align:-.93em"></span><span class="mord mathnormal">s</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:.10764em">f</span><span class="mord mathnormal">t</span><span class="mord mathnormal">m</span><span class="mord mathnormal">a</span><span class="mord mathnormal">x</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5183309999999999em"><span style="top:-2.25278em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.85722em"><span class="svg-align" style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord" style="padding-left:.833em"><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.33610799999999996em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.03148em">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span><span style="top:-2.81722em"><span class="pstrut" style="height:3em"></span><span class="hide-tail" style="min-width:.853em;height:1.08em"><svg width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.18278000000000005em"><span></span></span></span></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right:.07153em">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8413309999999999em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.13889em">T</span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.93em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span><span class="mord mathnormal" style="margin-right:.22222em">V</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.03588em">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31166399999999994em"><span style="top:-2.5500000000000003em;margin-left:-.03588em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.969438em;vertical-align:-.286108em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.15139200000000003em"><span style="top:-2.5500000000000003em;margin-left:-.13889em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.03588em">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.286108em"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31166399999999994em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:.84444em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.03148em">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31166399999999994em"><span style="top:-2.5500000000000003em;margin-left:-.03148em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.83333em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.33610799999999996em"><span style="top:-2.5500000000000003em;margin-left:-.13889em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.03148em">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31166399999999994em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:.58056em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.03588em">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31166399999999994em"><span style="top:-2.5500000000000003em;margin-left:-.03588em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.83333em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.151392em"><span style="top:-2.5500000000000003em;margin-left:-.13889em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.03588em">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31166399999999994em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span><span class="mspace newline"></span></span></span></span></p><h3 id="decoder"><a class="markdownIt-Anchor" href="#decoder"></a> Decoder</h3><p>Keys and values are computed from encoder embeddings</p><p>Queries are computed from decoder embeddings</p><h3 id="masked-multi-head-attention"><a class="markdownIt-Anchor" href="#masked-multi-head-attention"></a> Masked multi-head attention</h3><p>in the decoder the self-attention layer is only allowed to attend to earlier positions in the output sequence. This is done by masking future positions before the softmax step in the self-attention calculation.</p><p>positional encoding: usually works fine if they are hard-coded</p><p>simple hard-coded solution: rotational positional encoding vectors</p><h2 id="rnn-vs-transformer"><a class="markdownIt-Anchor" href="#rnn-vs-transformer"></a> RNN vs Transformer</h2><p>RNN:</p><p>✅ LSTM work reasonably well for long sequences.</p><p>⛔ expects an ordered sequence of inputs</p><p>⛔ subsequent hidden states can only be computed after the previous ones are done.</p><p>Transformer:</p><p>✅ good at long sequences</p><p>✅ can operate over unordered sets or ordered sequences with positional encodings</p><p>✅ parallel computation: all alignment and attention scores for all inputs can be done in parallel</p><p>⛔ require a lot of memory: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>×</mo><mi>M</mi></mrow><annotation encoding="application/x-tex">N\times M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.76666em;vertical-align:-.08333em"></span><span class="mord mathnormal" style="margin-right:.10903em">N</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.10903em">M</span></span></span></span> alignment and attention scalers need to be calculated and stored for a single self-attention head.</p><h1 id="some-sota-model"><a class="markdownIt-Anchor" href="#some-sota-model"></a> Some SOTA Model</h1><h2 id="gpt-3"><a class="markdownIt-Anchor" href="#gpt-3"></a> GPT-3</h2><p>transformer-based, large language model with 175M parameters</p><p>trained to perform next word prediction.</p><p>trained on</p><ul><li>common crawl dataset</li><li>WebText dataset collected by scraping links over a longer period of time</li><li>two Internet-based books corpora</li><li>English-language Wikipedia</li></ul><p>Before GPT-3:</p><ul><li><p>pre-train language model to perform next word prediction or masked word prediction</p></li><li><p>then fine-tune on labeled dataset to solve specific downstream task</p></li></ul><h3 id="in-context-learning"><a class="markdownIt-Anchor" href="#in-context-learning"></a> In-context learning</h3><p>unsupervised pre-training</p><p>prompting</p><p>zero-shot</p><ul><li>the model predicts the answer given only a natural language description of the task. no gradient update are performed.</li></ul><p>one-shot</p><ul><li>in addition to the task description, the model sees a single example of the task. no gradient update.</li></ul><p>few-shot</p><ul><li>in addition to task description, the model sees a few examples of the task. no gradient update.</li></ul><h3 id="github-copilot"><a class="markdownIt-Anchor" href="#github-copilot"></a> GitHub Copilot</h3><p>it uses OpenAI Codex to suggest code and entire functions in real-time, right from your editor</p><h2 id="other-models"><a class="markdownIt-Anchor" href="#other-models"></a> Other models</h2><h3 id="side-note-self-supervised-learning"><a class="markdownIt-Anchor" href="#side-note-self-supervised-learning"></a> Side-note: self-supervised learning</h3><h3 id="image-captioning-using-transformers"><a class="markdownIt-Anchor" href="#image-captioning-using-transformers"></a> image captioning using Transformers</h3><h3 id="vision-transformer-aka-vit"><a class="markdownIt-Anchor" href="#vision-transformer-aka-vit"></a> Vision Transformer aka ViT</h3><ul><li>image is split into fixed size patches, each of them are then linearly embedded, position embeddings are added, and the resulting sequence of vectors is fed to a standard Transformer encoder. In order to perform classification, a standard approach of adding an extra learnable “classification token” to the sequence is used.</li></ul><h3 id="contrastive-language-image-pre-training"><a class="markdownIt-Anchor" href="#contrastive-language-image-pre-training"></a> Contrastive Language-Image Pre-Training</h3><p>aka CLIP</p><p>learns association between an image and a piece of text</p><p>this can be achieved by training two separate transformer encoders for text snippet and the image.</p><p>the encoded image as well as the text features can then be compared for their respective similarity by constructing a cosine-similarity matrix</p><h3 id="dall-e-and-stable-diffusion"><a class="markdownIt-Anchor" href="#dall-e-and-stable-diffusion"></a> DALL-E and Stable Diffusion</h3><p>use text encoding from CLIP as the target encoding</p><p>encoding 用 CLIP，解码用 Inverting CNN</p><p>Stable Diffusion: the image information creator makes two inputs</p><ol><li>text embedding</li><li>random noise</li></ol><p>why not use GAN? Because it lacks of diversity in image generation, mode collapse, hard to train etc.</p><p>Diffusion models - basic principle</p><ul><li>forward diffusion process: the image is corrupted by gradually introducing noise until the image becomes complete random noise.</li><li>reverse diffusion process: in the reverse process, a series of Markov Chains are used to recover the data from the Gaussian noise by gradually removing the predicted noise at each time step.</li></ul><h3 id="latent-diffusion-models"><a class="markdownIt-Anchor" href="#latent-diffusion-models"></a> Latent Diffusion Models</h3><p>operate in the pixel space are demanding in terms of compute and memory</p><p>an LDM applies the diffusion processes in the latent space instead of pixel space while incorporating semantic feedback from a Transformer</p><p>LDMs are not only memory efficient but also produce diverse, highly detailed images which preserve the semantic structure of the data.</p><h3 id="conditioned-latent-diffusion"><a class="markdownIt-Anchor" href="#conditioned-latent-diffusion"></a> Conditioned Latent Diffusion</h3><p>the image generation task is conditioned on a prior</p><p>denoising U-Net autoencoder with cross-attention</p><h3 id="multimodal-machine-learning"><a class="markdownIt-Anchor" href="#multimodal-machine-learning"></a> Multimodal Machine Learning</h3><p>Transformers work on any kind of sequence data - just requires some kind of embedding</p><p>multimodal learning refers to the process of learning representations from different types of modalities using the same model</p></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="https://tinsir888.github.io">tinsir888</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="https://tinsir888.github.io/posts/e7117333.html">https://tinsir888.github.io/posts/e7117333.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener external nofollow noreferrer" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E7%AC%94%E8%AE%B0/">笔记</a><a class="post-meta__tags" href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/">计算机视觉</a></div><div class="post_share"><div class="social-share" data-image="/img/AUcv.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/78958afd.html" title="算法博弈论 W47 Congestion games, potential functions"><img class="cover" src="/img/AUagt.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">Previous</div><div class="prev_info">算法博弈论 W47 Congestion games, potential functions</div></div></a></div><div class="next-post pull-right"><a href="/posts/955dd42e.html" title="计算几何 W46 More Orthogonal problems"><img class="cover" src="/img/AUcg.png" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">Next</div><div class="next_info">计算几何 W46 More Orthogonal problems</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/posts/48cec433.html" title="计算机视觉 W48 Roundup - Uncovered Areas (新课完结)"><img class="cover" src="/img/AUcv.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-11-27</div><div class="title">计算机视觉 W48 Roundup - Uncovered Areas (新课完结)</div></div></a></div><div><a href="/posts/61b1055e.html" title="计算机视觉 W46 Visualizing and understanding CNNs"><img class="cover" src="/img/AUcv.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-11-13</div><div class="title">计算机视觉 W46 Visualizing and understanding CNNs</div></div></a></div><div><a href="/posts/56ab7808.html" title="计算机视觉 W45 Generative models"><img class="cover" src="/img/AUcv.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-11-06</div><div class="title">计算机视觉 W45 Generative models</div></div></a></div><div><a href="/posts/4802d8c1.html" title="计算机视觉 W44 Object detection and segmentation"><img class="cover" src="/img/AUcv.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-10-30</div><div class="title">计算机视觉 W44 Object detection and segmentation</div></div></a></div><div><a href="/posts/7a6ff316.html" title="计算机视觉 W43 CNN architectures"><img class="cover" src="/img/AUcv.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-10-23</div><div class="title">计算机视觉 W43 CNN architectures</div></div></a></div><div><a href="/posts/4d54f26b.html" title="计算机视觉 W41 Training ConvNet (2)"><img class="cover" src="/img/AUcv.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-10-09</div><div class="title">计算机视觉 W41 Training ConvNet (2)</div></div></a></div></div></div><hr class="custom-hr"><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> Comment</span></div><div class="comment-switch"><span class="first-comment">Giscus</span><span id="switch-btn"></span><span class="second-comment">Gitalk</span></div></div><div class="comment-wrap"><div><div id="giscus-wrap"></div></div><div><div id="gitalk-container"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/favicon.png" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"></div><div class="author-info__name">tinsir888</div><div class="author-info__description">Elysium :-)</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">316</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">74</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">32</div></a></div><a id="card-info-btn" target="_blank" rel="noopener external nofollow noreferrer" href="https://www.github.com/tinsir888"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/tinsir888" rel="external nofollow noreferrer" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:smalltalk.odin@protonmail.com" rel="external nofollow noreferrer" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="/atom.xml" target="_blank" title="RSS"><i class="fas fa-rss"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">Ηλύσια Πεδία ;-)</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#sequence-models"><span class="toc-number">1.</span> <span class="toc-text">Sequence Models</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#nlp"><span class="toc-number">2.</span> <span class="toc-text">NLP</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#language-model-next-word-prediction"><span class="toc-number">2.1.</span> <span class="toc-text">Language model ≈ next word prediction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#word-embeddings"><span class="toc-number">2.2.</span> <span class="toc-text">Word embeddings</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#sentence-embeddings"><span class="toc-number">2.3.</span> <span class="toc-text">Sentence embeddings</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#basic-modeling-paradigm"><span class="toc-number">2.4.</span> <span class="toc-text">Basic modeling paradigm</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#some-sequence-model"><span class="toc-number">3.</span> <span class="toc-text">Some sequence model</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#recurrent-neural-network"><span class="toc-number">3.1.</span> <span class="toc-text">Recurrent Neural Network</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#training-rnns"><span class="toc-number">3.1.1.</span> <span class="toc-text">Training RNNs</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#sentence-embeddings-with-rnnlstm"><span class="toc-number">3.1.2.</span> <span class="toc-text">Sentence embeddings with RNN&#x2F;LSTM</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#language-modeling"><span class="toc-number">3.2.</span> <span class="toc-text">Language modeling</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#sequence-to-sequence-models"><span class="toc-number">3.3.</span> <span class="toc-text">Sequence-to-sequence models</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#problem-with-seq2seq-models"><span class="toc-number">3.3.1.</span> <span class="toc-text">Problem with Seq2seq models</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#solution-attention"><span class="toc-number">3.4.</span> <span class="toc-text">Solution: Attention</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#attention-in-image-captioning"><span class="toc-number">3.4.1.</span> <span class="toc-text">Attention in image Captioning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#self-attention"><span class="toc-number">3.4.2.</span> <span class="toc-text">Self-attention</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#transformers"><span class="toc-number">3.5.</span> <span class="toc-text">Transformers</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#encoder-block"><span class="toc-number">3.5.1.</span> <span class="toc-text">Encoder block</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#self-attention-block"><span class="toc-number">3.5.2.</span> <span class="toc-text">Self-attention block</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#decoder"><span class="toc-number">3.5.3.</span> <span class="toc-text">Decoder</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#masked-multi-head-attention"><span class="toc-number">3.5.4.</span> <span class="toc-text">Masked multi-head attention</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#rnn-vs-transformer"><span class="toc-number">3.6.</span> <span class="toc-text">RNN vs Transformer</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#some-sota-model"><span class="toc-number">4.</span> <span class="toc-text">Some SOTA Model</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#gpt-3"><span class="toc-number">4.1.</span> <span class="toc-text">GPT-3</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#in-context-learning"><span class="toc-number">4.1.1.</span> <span class="toc-text">In-context learning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#github-copilot"><span class="toc-number">4.1.2.</span> <span class="toc-text">GitHub Copilot</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#other-models"><span class="toc-number">4.2.</span> <span class="toc-text">Other models</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#side-note-self-supervised-learning"><span class="toc-number">4.2.1.</span> <span class="toc-text">Side-note: self-supervised learning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#image-captioning-using-transformers"><span class="toc-number">4.2.2.</span> <span class="toc-text">image captioning using Transformers</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#vision-transformer-aka-vit"><span class="toc-number">4.2.3.</span> <span class="toc-text">Vision Transformer aka ViT</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#contrastive-language-image-pre-training"><span class="toc-number">4.2.4.</span> <span class="toc-text">Contrastive Language-Image Pre-Training</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#dall-e-and-stable-diffusion"><span class="toc-number">4.2.5.</span> <span class="toc-text">DALL-E and Stable Diffusion</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#latent-diffusion-models"><span class="toc-number">4.2.6.</span> <span class="toc-text">Latent Diffusion Models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#conditioned-latent-diffusion"><span class="toc-number">4.2.7.</span> <span class="toc-text">Conditioned Latent Diffusion</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#multimodal-machine-learning"><span class="toc-number">4.2.8.</span> <span class="toc-text">Multimodal Machine Learning</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/posts/a1455432.html" title="【整活】疑似 2025 年高考全国卷外语（丹麦语）试卷流出"><img src="/img/dansk-du3.png" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="【整活】疑似 2025 年高考全国卷外语（丹麦语）试卷流出"></a><div class="content"><a class="title" href="/posts/a1455432.html" title="【整活】疑似 2025 年高考全国卷外语（丹麦语）试卷流出">【整活】疑似 2025 年高考全国卷外语（丹麦语）试卷流出</a><time datetime="2025-06-06T22:00:00.000Z" title="Created 2025-06-07 00:00:00">2025-06-07</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/253981c7.html" title="丹麦语 DU 3.4 7 Forberedelse til modultest 4"><img src="/img/dansk-du34.png" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="丹麦语 DU 3.4 7 Forberedelse til modultest 4"></a><div class="content"><a class="title" href="/posts/253981c7.html" title="丹麦语 DU 3.4 7 Forberedelse til modultest 4">丹麦语 DU 3.4 7 Forberedelse til modultest 4</a><time datetime="2025-06-01T22:00:00.000Z" title="Created 2025-06-02 00:00:00">2025-06-02</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/6663e652.html" title="丹麦语 DU 3.4 6 Noget om tøj, budskaber og bæredygtighed"><img src="/img/dansk-du34.png" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="丹麦语 DU 3.4 6 Noget om tøj, budskaber og bæredygtighed"></a><div class="content"><a class="title" href="/posts/6663e652.html" title="丹麦语 DU 3.4 6 Noget om tøj, budskaber og bæredygtighed">丹麦语 DU 3.4 6 Noget om tøj, budskaber og bæredygtighed</a><time datetime="2025-04-28T22:00:00.000Z" title="Created 2025-04-29 00:00:00">2025-04-29</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/4784ce80.html" title="丹麦语 DU 3.4 5 Noget om arbejde"><img src="/img/dansk-du34.png" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="丹麦语 DU 3.4 5 Noget om arbejde"></a><div class="content"><a class="title" href="/posts/4784ce80.html" title="丹麦语 DU 3.4 5 Noget om arbejde">丹麦语 DU 3.4 5 Noget om arbejde</a><time datetime="2025-04-24T22:00:00.000Z" title="Created 2025-04-25 00:00:00">2025-04-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/9de6d385.html" title="丹麦语 DU 3.4 4 Noget om køn og lidt om alder"><img src="/img/dansk-du34.png" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="丹麦语 DU 3.4 4 Noget om køn og lidt om alder"></a><div class="content"><a class="title" href="/posts/9de6d385.html" title="丹麦语 DU 3.4 4 Noget om køn og lidt om alder">丹麦语 DU 3.4 4 Noget om køn og lidt om alder</a><time datetime="2025-04-08T22:00:00.000Z" title="Created 2025-04-09 00:00:00">2025-04-09</time></div></div></div></div></div></div></main><footer id="footer" style="background-image:url(/img/AUcv.jpg)"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By tinsir888</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a><span class="footer-separator">|</span><span> </span><a href="https://icp.gov.moe/?keyword=20248537" rel="external nofollow noreferrer" target="_blank"><img src="https://icp.gov.moe/images/ico64.png" alt="icon" width="16" height="16" style="vertical-align:middle;margin-right:5px">萌ICP备20248537号</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="Scroll To Comments"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>document.querySelectorAll("#article-container span.katex-display").forEach(a=>{btf.wrap(a,"div",{class:"katex-wrap"})})</script><script>(()=>{const e=document.querySelectorAll("#article-container .mermaid-wrap");if(0===e.length)return;const t=()=>{window.loadMermaid=!0;const t="dark"===document.documentElement.getAttribute("data-theme")?"dark":"default";Array.from(e).forEach((e,n)=>{const d=e.firstElementChild,r="mermaid-"+n,a="%%{init:{ 'theme':'"+t+"'}}%%\n"+d.textContent,i=mermaid.render(r,a);var m;"string"==typeof i?(m=i,d.insertAdjacentHTML("afterend",m)):i.then(({svg:e})=>{d.insertAdjacentHTML("afterend",e)})})},n=()=>{window.loadMermaid?t():getScript("https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js").then(t)};btf.addGlobalFn("themeChange",t,"mermaid"),window.pjax?n():document.addEventListener("DOMContentLoaded",n)})()</script><script>(()=>{const t=t=>"dark"===t?"dark":"light",e=()=>{const e=Object.assign({src:"https://giscus.app/client.js","data-repo":"tinsir888/tinsir888.github.io-gittalk","data-repo-id":"R_kgDOLoNheQ","data-category-id":"DIC_kwDOLoNhec4Cktkv","data-mapping":"pathname","data-theme":t(document.documentElement.getAttribute("data-theme")),"data-reactions-enabled":"1",crossorigin:"anonymous",async:!0},null),a=document.createElement("script");for(let t in e)a.setAttribute(t,e[t]);document.getElementById("giscus-wrap").appendChild(a)};btf.addGlobalFn("themeChange",e=>{(t=>{const e=document.querySelector("iframe.giscus-frame");e&&e.contentWindow.postMessage({giscus:t},"https://giscus.app")})({setConfig:{theme:t(e)}})},"giscus"),e()})()</script><script>(()=>{const t=()=>{new Gitalk(Object.assign({clientID:"e1723135269b287b0966",clientSecret:"b3eb09deb82a253cc57efd054834f5dac3354cb8",repo:"tinsir888.github.io-gittalk",owner:"tinsir888",admin:["tinsir888"],id:"7396063fd2f1db135478db2ad15c0d24",updateCountCallback:n},null)).render("gitalk-container")},i=async()=>{"function"==typeof Gitalk||(await getCSS("https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css"),await getScript("https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.js")),t()},n=t=>{const i=document.querySelector("#post-meta .gitalk-comment-count");i&&(i.textContent=t)};i()})()</script></div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/fireworks.min.js"></script><script defer id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-fluttering-ribbon.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful=!0,POWERMODE.shake=!0,POWERMODE.mobile=!1,document.body.addEventListener("input",POWERMODE)</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span> Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>